{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44d516c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AzureExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "print(ort.get_available_providers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c162dabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/12/25 22:01:06] </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">ERROR   </span> Unknown resize method, defaulting to <span style=\"color: #008000; text-decoration-color: #008000\">'Stretch to'</span> - this may result in <a href=\"file://c:\\Users\\Shane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\inference\\core\\models\\roboflow.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">roboflow.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\Shane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\inference\\core\\models\\roboflow.py#478\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">478</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         degraded model performance.                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/12/25 22:01:06]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;31mERROR   \u001b[0m Unknown resize method, defaulting to \u001b[32m'Stretch to'\u001b[0m - this may result in \u001b]8;id=333023;file://c:\\Users\\Shane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\inference\\core\\models\\roboflow.py\u001b\\\u001b[2mroboflow.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=15516;file://c:\\Users\\Shane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\inference\\core\\models\\roboflow.py#478\u001b\\\u001b[2m478\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         degraded model performance.                                            \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Convert results to Supervision Detections\u001b[39;00m\n\u001b[32m     31\u001b[39m detections = sv.Detections.from_inference(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\inference\\core\\models\\object_detection_base.py:84\u001b[39m, in \u001b[36mObjectDetectionBaseOnnxRoboflowInferenceModel.infer\u001b[39m\u001b[34m(self, image, class_agnostic_nms, confidence, disable_preproc_auto_orient, disable_preproc_contrast, disable_preproc_grayscale, disable_preproc_static_crop, iou_threshold, fix_batch_size, max_candidates, max_detections, return_image_dims, **kwargs)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minfer\u001b[39m(\n\u001b[32m     43\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     44\u001b[39m     image: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m     56\u001b[39m     **kwargs,\n\u001b[32m     57\u001b[39m ) -> Any:\n\u001b[32m     58\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[33;03m    Runs object detection inference on one or multiple images and returns the detections.\u001b[39;00m\n\u001b[32m     60\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     82\u001b[39m \u001b[33;03m        ValueError: If batching is not enabled for the model and more than one image is passed for processing.\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_agnostic_nms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_agnostic_nms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfidence\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfidence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable_preproc_auto_orient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_preproc_auto_orient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable_preproc_contrast\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_preproc_contrast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable_preproc_grayscale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_preproc_grayscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable_preproc_static_crop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_preproc_static_crop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfix_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfix_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_candidates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_detections\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_detections\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_image_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_image_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\inference\\core\\models\\roboflow.py:821\u001b[39m, in \u001b[36mOnnxRoboflowInferenceModel.infer\u001b[39m\u001b[34m(self, image, **kwargs)\u001b[39m\n\u001b[32m    819\u001b[39m max_batch_size = MAX_BATCH_SIZE \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batching_enabled \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_size\n\u001b[32m    820\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (input_elements == \u001b[32m1\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (max_batch_size == \u001b[38;5;28mfloat\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33minf\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    822\u001b[39m logger.debug(\n\u001b[32m    823\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInference will be executed in batches, as there is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_elements\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m input elements and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    824\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmaximum batch size for a model is set to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    825\u001b[39m )\n\u001b[32m    826\u001b[39m inference_results = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\inference\\usage_tracking\\collector.py:715\u001b[39m, in \u001b[36mUsageCollector.__call__.<locals>.decorator.<locals>.sync_wrapper\u001b[39m\u001b[34m(usage_fps, usage_api_key, usage_workflow_id, usage_workflow_preview, usage_inference_test_run, usage_billable, *args, **kwargs)\u001b[39m\n\u001b[32m    713\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    714\u001b[39m     t1 = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m715\u001b[39m     res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    716\u001b[39m     t2 = time.time()\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m GCP_SERVERLESS \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\inference\\core\\models\\base.py:29\u001b[39m, in \u001b[36mBaseInference.infer\u001b[39m\u001b[34m(self, image, **kwargs)\u001b[39m\n\u001b[32m     25\u001b[39m preproc_image, returned_metadata = \u001b[38;5;28mself\u001b[39m.preprocess(image, **kwargs)\n\u001b[32m     26\u001b[39m logger.debug(\n\u001b[32m     27\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPreprocessed input shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mgetattr\u001b[39m(preproc_image,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     28\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m predicted_arrays = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreproc_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m postprocessed = \u001b[38;5;28mself\u001b[39m.postprocess(predicted_arrays, returned_metadata, **kwargs)\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m postprocessed\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\inference\\models\\yolov8\\yolov8_object_detection.py:45\u001b[39m, in \u001b[36mYOLOv8ObjectDetection.predict\u001b[39m\u001b[34m(self, img_in, **kwargs)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Performs object detection on the given image using the ONNX session.\u001b[39;00m\n\u001b[32m     37\u001b[39m \n\u001b[32m     38\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m \u001b[33;03m    Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._session_lock:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     predictions = \u001b[43mrun_session_via_iobinding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43monnx_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_in\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     48\u001b[39m predictions = predictions.transpose(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m     49\u001b[39m boxes = predictions[:, :, :\u001b[32m4\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\inference\\core\\utils\\onnx.py:36\u001b[39m, in \u001b[36mrun_session_via_iobinding\u001b[39m\u001b[34m(session, input_name, input_data)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_session_via_iobinding\u001b[39m(\n\u001b[32m     31\u001b[39m     session: ort.InferenceSession, input_name: \u001b[38;5;28mstr\u001b[39m, input_data: ImageMetaType\n\u001b[32m     32\u001b[39m ) -> List[np.ndarray]:\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(input_data, (np.ndarray, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[32m     34\u001b[39m         \u001b[38;5;66;03m# skip the iobinding and just run the session\u001b[39;00m\n\u001b[32m     35\u001b[39m         \u001b[38;5;66;03m# we likely won't get any gains by pointing to the input data directly\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         predictions = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCUDAExecutionProvider\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m session.get_providers():\n\u001b[32m     38\u001b[39m         \u001b[38;5;66;03m# no point in doing iobinding as the input must live on CPU anyway\u001b[39;00m\n\u001b[32m     39\u001b[39m         input_data = (\n\u001b[32m     40\u001b[39m             input_data.cpu().numpy()\n\u001b[32m     41\u001b[39m         )  \u001b[38;5;66;03m# since we must be a tensor but ONNX needs a numpy array\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:270\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(self, output_names, input_feed, run_options)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, output_names, input_feed, run_options=\u001b[38;5;28;01mNone\u001b[39;00m) -> Sequence[np.ndarray | SparseTensor | \u001b[38;5;28mlist\u001b[39m | \u001b[38;5;28mdict\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    271\u001b[39m \u001b[33;03m    Compute the predictions.\u001b[39;00m\n\u001b[32m    272\u001b[39m \n\u001b[32m    273\u001b[39m \u001b[33;03m    :param output_names: name of the outputs\u001b[39;00m\n\u001b[32m    274\u001b[39m \u001b[33;03m    :param input_feed: dictionary ``{ input_name: input_value }``\u001b[39;00m\n\u001b[32m    275\u001b[39m \u001b[33;03m    :param run_options: See :class:`onnxruntime.RunOptions`.\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[33;03m    :return: list of results, every result is either a numpy array,\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m        a sparse tensor, a list or a dictionary.\u001b[39;00m\n\u001b[32m    278\u001b[39m \n\u001b[32m    279\u001b[39m \u001b[33;03m    ::\u001b[39;00m\n\u001b[32m    280\u001b[39m \n\u001b[32m    281\u001b[39m \u001b[33;03m        sess.run([output_name], {input_name: x})\u001b[39;00m\n\u001b[32m    282\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_input(\u001b[38;5;28mlist\u001b[39m(input_feed.keys()))\n\u001b[32m    284\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_names:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from inference import get_model\n",
    "import supervision as sv\n",
    "import cv2\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables and API key\n",
    "load_dotenv()\n",
    "ROBOFLOW_API_KEY = os.getenv(\"ROBOFLOW_API_KEY\")\n",
    "\n",
    "# Load your trained model from Roboflow\n",
    "model = get_model(model_id=\"test-ns29a/3\", api_key=ROBOFLOW_API_KEY)\n",
    "\n",
    "# Initialize webcam (0 = default webcam)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Supervision annotators\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "    \n",
    "    # Run inference\n",
    "    results = model.infer(frame)[0]\n",
    "    \n",
    "    # Convert results to Supervision Detections\n",
    "    detections = sv.Detections.from_inference(results)\n",
    "    \n",
    "    # Annotate frame\n",
    "    annotated_frame = box_annotator.annotate(scene=frame.copy(), detections=detections)\n",
    "    annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "    \n",
    "    # Show frame\n",
    "    cv2.imshow(\"Real-time Detection\", annotated_frame)\n",
    "    \n",
    "    # Break when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
